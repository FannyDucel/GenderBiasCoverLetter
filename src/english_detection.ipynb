{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of models which contains english generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import fasttext\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Dl GlotLID model\n",
    "model_path = hf_hub_download(repo_id=\"cis-lmu/glotlid\", filename=\"model.bin\")\n",
    "model = fasttext.load_model(model_path)\n",
    "\n",
    "# Fonction which returns True if the text is in English\n",
    "def is_english(text, threshold=0.8):\n",
    "    if pd.isna(text) or not isinstance(text, str) or text.strip() == \"\":\n",
    "        return False\n",
    "    try:\n",
    "        text = text.replace(\"\\n\", \" \").strip()  # remove newlines because of a error \n",
    "        label, prob = model.predict(text)\n",
    "        lang_code = label[0].replace(\"__label__\", \"\")\n",
    "        return lang_code.startswith(\"eng\") and prob[0] >= threshold\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {text[:50]}... -> {e}\")\n",
    "        return False\n",
    "\n",
    "# analyze\n",
    "def analyze_english_percentage(setting):\n",
    "    modeles = [\n",
    "        \"bloom-560m\", \"bloom-3b\", \"gpt2-fr\", \"xglm-2\", \"bloom-7b\",\n",
    "        \"vigogne-2-7b\", \"croissantbase\", \"croissant-it\",\n",
    "        \"llama-3.2-3b-it\", \"llama-3.2-3b\",\n",
    "        \"gemma-2-2b\", \"gemma-2-2b-it\",\n",
    "        \"mistral-7b-instruct-v0.3\", \"mistral-7b-v0.3\"\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    for modele in modeles:\n",
    "        print(f\"Model : {modele}\")\n",
    "        csv_file = f\"../annotated_texts/FR/{setting}/annotated-coverletter_{setting}_fr_{modele}.csv\"\n",
    "        df = pd.read_csv(csv_file)\n",
    "\n",
    "        column_name = \"output\"\n",
    "\n",
    "        tqdm.pandas(desc=f\"Analyzing sentences for : {modele}\")\n",
    "        df[\"is_english\"] = df[column_name].progress_apply(lambda x: is_english(x))\n",
    "\n",
    "        english_count = df[\"is_english\"].sum()\n",
    "        total_count = len(df)\n",
    "        percentage_english = (english_count / total_count) * 100\n",
    "\n",
    "        results.append({\n",
    "            \"Rank\": None,\n",
    "            \"Model\": modele,\n",
    "            \"English_percentage\": round(percentage_english, 2)\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df[\"Rank\"] = results_df[\"English_percentage\"].rank(ascending=False, method=\"first\").astype(int)\n",
    "    results_df = results_df.sort_values(by=\"English_percentage\", ascending=False).reset_index(drop=True)  \n",
    "    results_df[\"English_percentage\"] = results_df[\"English_percentage\"].map(\"{:.2f}\".format)\n",
    "    print(results_df)\n",
    "\n",
    "    #output_file = f\"./results_percentage_english_{setting}.csv\"\n",
    "    #results_df.to_csv(output_file, index=False)\n",
    "    #print(f\"Résultats sauvegardés dans {output_file}\")\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : bloom-560m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing sentences for : bloom-560m: 100%|██████████| 4992/4992 [00:03<00:00, 1436.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : bloom-3b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing sentences for : bloom-3b: 100%|██████████| 4992/4992 [00:04<00:00, 1208.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : gpt2-fr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing sentences for : gpt2-fr: 100%|██████████| 5006/5006 [00:04<00:00, 1131.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : xglm-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing sentences for : xglm-2: 100%|██████████| 4968/4968 [00:02<00:00, 1713.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : bloom-7b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing sentences for : bloom-7b: 100%|██████████| 4998/4998 [00:04<00:00, 1218.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : vigogne-2-7b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing sentences for : vigogne-2-7b: 100%|██████████| 4992/4992 [00:03<00:00, 1327.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : croissantbase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing sentences for : croissantbase: 100%|██████████| 4968/4968 [00:03<00:00, 1266.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : croissant-it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing sentences for : croissant-it: 100%|██████████| 4968/4968 [00:04<00:00, 1073.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : llama-3.2-3b-it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing sentences for : llama-3.2-3b-it: 100%|██████████| 4968/4968 [00:04<00:00, 1180.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : llama-3.2-3b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing sentences for : llama-3.2-3b: 100%|██████████| 4968/4968 [00:04<00:00, 1159.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing sentences for : gemma-2-2b: 100%|██████████| 4968/4968 [00:04<00:00, 1226.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : gemma-2-2b-it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing sentences for : gemma-2-2b-it: 100%|██████████| 4968/4968 [00:04<00:00, 1101.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : mistral-7b-instruct-v0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing sentences for : mistral-7b-instruct-v0.3: 100%|██████████| 4992/4992 [00:03<00:00, 1313.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : mistral-7b-v0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing sentences for : mistral-7b-v0.3: 100%|██████████| 4992/4992 [00:03<00:00, 1353.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Rank                     Model English_percentage\n",
      "0      1           mistral-7b-v0.3              40.16\n",
      "1      2  mistral-7b-instruct-v0.3               3.65\n",
      "2      3             gemma-2-2b-it               1.11\n",
      "3      4              croissant-it               0.14\n",
      "4      5           llama-3.2-3b-it               0.08\n",
      "5      6              llama-3.2-3b               0.08\n",
      "6      7              vigogne-2-7b               0.06\n",
      "7      8             croissantbase               0.06\n",
      "8      9                   gpt2-fr               0.04\n",
      "9     10                  bloom-7b               0.02\n",
      "10    11                gemma-2-2b               0.02\n",
      "11    12                bloom-560m               0.00\n",
      "12    13                  bloom-3b               0.00\n",
      "13    14                    xglm-2               0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "res_neutral = analyze_english_percentage(\"neutral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : bloom-560m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing sentences for : bloom-560m: 100%|██████████| 4968/4968 [00:03<00:00, 1330.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : bloom-3b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing sentences for : bloom-3b: 100%|██████████| 4968/4968 [00:03<00:00, 1276.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : gpt2-fr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing sentences for : gpt2-fr: 100%|██████████| 4968/4968 [00:04<00:00, 1163.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : xglm-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing sentences for : xglm-2: 100%|██████████| 4968/4968 [00:02<00:00, 1903.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : bloom-7b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing sentences for : bloom-7b: 100%|██████████| 4968/4968 [00:03<00:00, 1300.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : vigogne-2-7b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing sentences for : vigogne-2-7b: 100%|██████████| 4968/4968 [00:03<00:00, 1477.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : croissantbase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing sentences for : croissantbase: 100%|██████████| 4968/4968 [00:03<00:00, 1414.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : croissant-it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing sentences for : croissant-it: 100%|██████████| 4968/4968 [00:04<00:00, 1155.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : llama-3.2-3b-it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing sentences for : llama-3.2-3b-it: 100%|██████████| 4968/4968 [00:04<00:00, 1233.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : llama-3.2-3b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing sentences for : llama-3.2-3b: 100%|██████████| 4968/4968 [00:04<00:00, 1174.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing sentences for : gemma-2-2b: 100%|██████████| 4968/4968 [00:03<00:00, 1337.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : gemma-2-2b-it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing sentences for : gemma-2-2b-it: 100%|██████████| 4968/4968 [00:04<00:00, 1144.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : mistral-7b-instruct-v0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing sentences for : mistral-7b-instruct-v0.3: 100%|██████████| 4968/4968 [00:03<00:00, 1310.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : mistral-7b-v0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing sentences for : mistral-7b-v0.3: 100%|██████████| 4968/4968 [00:03<00:00, 1384.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Rank                     Model English_percentage\n",
      "0      1           mistral-7b-v0.3              48.71\n",
      "1      2  mistral-7b-instruct-v0.3               8.98\n",
      "2      3             gemma-2-2b-it               8.64\n",
      "3      4              vigogne-2-7b               0.34\n",
      "4      5             croissantbase               0.14\n",
      "5      6                gemma-2-2b               0.12\n",
      "6      7                   gpt2-fr               0.04\n",
      "7      8           llama-3.2-3b-it               0.04\n",
      "8      9              llama-3.2-3b               0.04\n",
      "9     10                bloom-560m               0.00\n",
      "10    11                  bloom-3b               0.00\n",
      "11    12                    xglm-2               0.00\n",
      "12    13                  bloom-7b               0.00\n",
      "13    14              croissant-it               0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "res_gendered= analyze_english_percentage(\"gendered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#res_gendered.to_latex(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#res_neutral.to_latex(index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenderBiasEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
