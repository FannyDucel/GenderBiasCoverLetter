{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of models which contains english generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import fasttext\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Télécharger et charger le modèle GlotLID \n",
    "model_path = hf_hub_download(repo_id=\"cis-lmu/glotlid\", filename=\"model.bin\")\n",
    "model = fasttext.load_model(model_path)\n",
    "\n",
    "# Fonction de détection de l'anglais avec GlotLID\n",
    "def is_english(text, threshold=0.8):\n",
    "    if pd.isna(text) or not isinstance(text, str) or text.strip() == \"\":\n",
    "        return False\n",
    "    try:\n",
    "        text = text.replace(\"\\n\", \" \").strip()  # remove newlines because of a error \n",
    "        label, prob = model.predict(text)\n",
    "        lang_code = label[0].replace(\"__label__\", \"\")\n",
    "        return lang_code.startswith(\"eng\") and prob[0] >= threshold\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {text[:50]}... -> {e}\")\n",
    "        return False\n",
    "\n",
    "# analyze\n",
    "def analyze_english_percentage(setting):\n",
    "    modeles = [\n",
    "        \"bloom-560m\", \"bloom-3b\", \"gpt2-fr\", \"xglm-2\", \"bloom-7b\",\n",
    "        \"vigogne-2-7b\", \"croissantbase\", \"croissant-it\",\n",
    "        \"llama-3.2-3b-it\", \"llama-3.2-3b\",\n",
    "        \"gemma-2-2b\", \"gemma-2-2b-it\",\n",
    "        \"mistral-7b-instruct-v0.3\", \"mistral-7b-v0.3\"\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    for modele in modeles:\n",
    "        print(f\"Traitement du modèle : {modele}\")\n",
    "        csv_file = f\"../annotated_texts/FR/{setting}/annotated-coverletter_{setting}_fr_{modele}.csv\"\n",
    "        df = pd.read_csv(csv_file)\n",
    "\n",
    "        column_name = \"output\"\n",
    "\n",
    "        tqdm.pandas(desc=f\"Analyse des phrases pour {modele}\")\n",
    "        df[\"is_english\"] = df[column_name].progress_apply(lambda x: is_english(x))\n",
    "\n",
    "        english_count = df[\"is_english\"].sum()\n",
    "        total_count = len(df)\n",
    "        percentage_english = (english_count / total_count) * 100\n",
    "\n",
    "        results.append({\n",
    "            \"Rank\": None,\n",
    "            \"Model\": modele,\n",
    "            \"English_percentage\": round(percentage_english, 2)\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df[\"Rank\"] = results_df[\"English_percentage\"].rank(ascending=False, method=\"first\").astype(int)\n",
    "    results_df = results_df.sort_values(by=\"English_percentage\", ascending=False).reset_index(drop=True)  \n",
    "    results_df[\"English_percentage\"] = results_df[\"English_percentage\"].map(\"{:.2f}\".format)\n",
    "    print(results_df)\n",
    "\n",
    "    #output_file = f\"./results_percentage_english_{setting}.csv\"\n",
    "    #results_df.to_csv(output_file, index=False)\n",
    "    #print(f\"Résultats sauvegardés dans {output_file}\")\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement du modèle : bloom-560m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyse des phrases pour bloom-560m: 100%|██████████| 4992/4992 [00:03<00:00, 1468.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement du modèle : bloom-3b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyse des phrases pour bloom-3b: 100%|██████████| 4992/4992 [00:03<00:00, 1265.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement du modèle : gpt2-fr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyse des phrases pour gpt2-fr: 100%|██████████| 5006/5006 [00:04<00:00, 1173.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement du modèle : xglm-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyse des phrases pour xglm-2: 100%|██████████| 4968/4968 [00:02<00:00, 1806.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement du modèle : bloom-7b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyse des phrases pour bloom-7b: 100%|██████████| 4998/4998 [00:03<00:00, 1276.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement du modèle : vigogne-2-7b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyse des phrases pour vigogne-2-7b: 100%|██████████| 4992/4992 [00:03<00:00, 1320.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement du modèle : croissantbase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyse des phrases pour croissantbase: 100%|██████████| 4968/4968 [00:03<00:00, 1297.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement du modèle : croissant-it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyse des phrases pour croissant-it: 100%|██████████| 4968/4968 [00:04<00:00, 1113.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement du modèle : llama-3.2-3b-it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyse des phrases pour llama-3.2-3b-it: 100%|██████████| 4968/4968 [00:04<00:00, 1163.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement du modèle : llama-3.2-3b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyse des phrases pour llama-3.2-3b: 100%|██████████| 4968/4968 [00:04<00:00, 1200.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement du modèle : gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyse des phrases pour gemma-2-2b: 100%|██████████| 4968/4968 [00:03<00:00, 1274.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement du modèle : gemma-2-2b-it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyse des phrases pour gemma-2-2b-it: 100%|██████████| 4968/4968 [00:04<00:00, 1137.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement du modèle : mistral-7b-instruct-v0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyse des phrases pour mistral-7b-instruct-v0.3: 100%|██████████| 4992/4992 [00:03<00:00, 1353.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement du modèle : mistral-7b-v0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyse des phrases pour mistral-7b-v0.3: 100%|██████████| 4992/4992 [00:03<00:00, 1400.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Rank                     Model English_percentage\n",
      "0      1           mistral-7b-v0.3              40.16\n",
      "1      2  mistral-7b-instruct-v0.3               3.65\n",
      "2      3             gemma-2-2b-it               1.11\n",
      "3      4              croissant-it               0.14\n",
      "4      5           llama-3.2-3b-it               0.08\n",
      "5      6              llama-3.2-3b               0.08\n",
      "6      7              vigogne-2-7b               0.06\n",
      "7      8             croissantbase               0.06\n",
      "8      9                   gpt2-fr               0.04\n",
      "9     10                  bloom-7b               0.02\n",
      "10    11                gemma-2-2b               0.02\n",
      "11    12                bloom-560m               0.00\n",
      "12    13                  bloom-3b               0.00\n",
      "13    14                    xglm-2               0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "res_neutral = analyze_english_percentage(\"neutral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement du modèle : bloom-560m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyse des phrases pour bloom-560m: 100%|██████████| 4968/4968 [00:03<00:00, 1346.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement du modèle : bloom-3b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyse des phrases pour bloom-3b: 100%|██████████| 4968/4968 [00:03<00:00, 1304.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement du modèle : gpt2-fr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyse des phrases pour gpt2-fr: 100%|██████████| 4968/4968 [00:04<00:00, 1188.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement du modèle : xglm-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyse des phrases pour xglm-2: 100%|██████████| 4968/4968 [00:02<00:00, 1957.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement du modèle : bloom-7b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyse des phrases pour bloom-7b: 100%|██████████| 4968/4968 [00:03<00:00, 1334.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement du modèle : vigogne-2-7b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyse des phrases pour vigogne-2-7b: 100%|██████████| 4968/4968 [00:03<00:00, 1463.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement du modèle : croissantbase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyse des phrases pour croissantbase: 100%|██████████| 4968/4968 [00:03<00:00, 1405.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement du modèle : croissant-it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyse des phrases pour croissant-it: 100%|██████████| 4968/4968 [00:04<00:00, 1184.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement du modèle : llama-3.2-3b-it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyse des phrases pour llama-3.2-3b-it: 100%|██████████| 4968/4968 [00:04<00:00, 1237.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement du modèle : llama-3.2-3b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyse des phrases pour llama-3.2-3b: 100%|██████████| 4968/4968 [00:03<00:00, 1254.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement du modèle : gemma-2-2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyse des phrases pour gemma-2-2b: 100%|██████████| 4968/4968 [00:03<00:00, 1405.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement du modèle : gemma-2-2b-it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyse des phrases pour gemma-2-2b-it: 100%|██████████| 4968/4968 [00:04<00:00, 1220.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement du modèle : mistral-7b-instruct-v0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyse des phrases pour mistral-7b-instruct-v0.3: 100%|██████████| 4968/4968 [00:03<00:00, 1380.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement du modèle : mistral-7b-v0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyse des phrases pour mistral-7b-v0.3: 100%|██████████| 4968/4968 [00:03<00:00, 1425.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Rank                     Model English_percentage\n",
      "0      1           mistral-7b-v0.3              48.71\n",
      "1      2  mistral-7b-instruct-v0.3               8.98\n",
      "2      3             gemma-2-2b-it               8.64\n",
      "3      4              vigogne-2-7b               0.34\n",
      "4      5             croissantbase               0.14\n",
      "5      6                gemma-2-2b               0.12\n",
      "6      7                   gpt2-fr               0.04\n",
      "7      8           llama-3.2-3b-it               0.04\n",
      "8      9              llama-3.2-3b               0.04\n",
      "9     10                bloom-560m               0.00\n",
      "10    11                  bloom-3b               0.00\n",
      "11    12                    xglm-2               0.00\n",
      "12    13                  bloom-7b               0.00\n",
      "13    14              croissant-it               0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "res_gendered= analyze_english_percentage(\"gendered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\begin{tabular}{rll}\\n\\\\toprule\\nRank & Model & English_percentage \\\\\\\\\\n\\\\midrule\\n1 & mistral-7b-v0.3 & 48.71 \\\\\\\\\\n2 & mistral-7b-instruct-v0.3 & 8.98 \\\\\\\\\\n3 & gemma-2-2b-it & 8.64 \\\\\\\\\\n4 & vigogne-2-7b & 0.34 \\\\\\\\\\n5 & croissantbase & 0.14 \\\\\\\\\\n6 & gemma-2-2b & 0.12 \\\\\\\\\\n7 & gpt2-fr & 0.04 \\\\\\\\\\n8 & llama-3.2-3b-it & 0.04 \\\\\\\\\\n9 & llama-3.2-3b & 0.04 \\\\\\\\\\n10 & bloom-560m & 0.00 \\\\\\\\\\n11 & bloom-3b & 0.00 \\\\\\\\\\n12 & xglm-2 & 0.00 \\\\\\\\\\n13 & bloom-7b & 0.00 \\\\\\\\\\n14 & croissant-it & 0.00 \\\\\\\\\\n\\\\bottomrule\\n\\\\end{tabular}\\n'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_gendered.to_latex(index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\begin{tabular}{rll}\\n\\\\toprule\\nRank & Model & English_percentage \\\\\\\\\\n\\\\midrule\\n1 & mistral-7b-v0.3 & 40.16 \\\\\\\\\\n2 & mistral-7b-instruct-v0.3 & 3.65 \\\\\\\\\\n3 & gemma-2-2b-it & 1.11 \\\\\\\\\\n4 & croissant-it & 0.14 \\\\\\\\\\n5 & llama-3.2-3b-it & 0.08 \\\\\\\\\\n6 & llama-3.2-3b & 0.08 \\\\\\\\\\n7 & vigogne-2-7b & 0.06 \\\\\\\\\\n8 & croissantbase & 0.06 \\\\\\\\\\n9 & gpt2-fr & 0.04 \\\\\\\\\\n10 & bloom-7b & 0.02 \\\\\\\\\\n11 & gemma-2-2b & 0.02 \\\\\\\\\\n12 & bloom-560m & 0.00 \\\\\\\\\\n13 & bloom-3b & 0.00 \\\\\\\\\\n14 & xglm-2 & 0.00 \\\\\\\\\\n\\\\bottomrule\\n\\\\end{tabular}\\n'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_neutral.to_latex(index =False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenderBiasEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
